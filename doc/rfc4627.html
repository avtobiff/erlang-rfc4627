<html>
  <head>
    <title>Documentation for rfc4627 for Erlang</title>
  </head>
  <body>
    <h1>rfc4627, the JSON codec</h1>

    <p>
      Links:
    </p>

    <ul>
      <li><a href="http://www.ietf.org/rfc/rfc4627.txt">RFC 4627</a></li>
      <li><a href="http://www.json.org/">JSON in general</a></li>
    </ul>

    <p>
      Basic API:
    </p>

    <pre>encode(val()) -> str()
decode(str()) -> {ok, val(), str()} | {error, Reason}
                 where Reason is usually far too much information
                 and should be ignored.</pre>

    <p>
      The data type mapping I've implemented is as per Joe Armstrong's
      message <a
      href="http://www.erlang.org/ml-archive/erlang-questions/200511/msg00193.html">http://www.erlang.org/ml-archive/erlang-questions/200511/msg00193.html</a>:
    </p>

    <pre>    JSON Obj    = type obj()   = {obj, [{key(), val()}]}
    JSON Array  = type array() = [val()]
    JSON Number = type num()   = int() | float() 
    JSON String = type str()   = bin()
    JSON true false null       = true, false null (atoms)
    With Type val() = obj() | array() | num() | str() | true | false | null
    and key() being a str(). (Or a binary or atom, during JSON encoding.)</pre>

    <p>
      When serializing a string, if characters are found with codepoint
      &gt;127, we rely on the unicode encoder to build the proper byte
      sequence for transmission. We still use the \uXXXX escape for
      control characters (other than the RFC-specified specially
      recognised ones).
    </p>

    <p>
      decode/1 will autodetect the unicode encoding used, and any strings
      returned in the result as binaries will contain UTF-8 encoded byte
      sequences for codepoints &gt;127. Object keys containing codepoints
      &gt;127 will be returned as lists of codepoints, rather than being
      UTF-8 encoded. If you have already transformed the text to parse
      into a list of unicode codepoints, perhaps by your own use of
      unicode_decode/1, then use decode_noauto/1 to avoid redundant and
      erroneous double-unicode-decoding.
    </p>

    <p>
      Similarly, encode/1 produces text that is already UTF-8 encoded. To
      get raw codepoints, use encode_noauto/1 and encode_noauto/2. You
      can use unicode_encode/1 to UTF-encode the results, if that's
      appropriate for your application.
    </p>

    <p>
      I'm lenient in the following ways during parsing:
    </p>

    <ul>
      <li>repeated commas in arrays and objects collapse to a single comma</li>
      <li>any character =&lt;32 is considered whitespace</li>
      <li>leading zeros for numbers are accepted</li>
      <li>
	we don't restrict the toplevel token to only object or array -
	any JSON value can be used at toplevel
      </li>
    </ul>
  </body>
</html>
